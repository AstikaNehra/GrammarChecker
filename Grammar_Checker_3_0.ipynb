{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grammar Checker 3.0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOm6Qa7zGXplkMcz8AKkwdb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AstikaNehra/GrammarChecker/blob/master/Grammar_Checker_3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVA-7UkPCrRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUWCCdPuC3S3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url_train = 'https://raw.githubusercontent.com/AstikaNehra/GrammarChecker/master/final_train.csv'\n",
        "url_test = 'https://raw.githubusercontent.com/AstikaNehra/GrammarChecker/master/final_test.csv'\n",
        "df_train = pd.read_csv(url_train,error_bad_lines=False, encoding='cp1252',header=None)\n",
        "df_test = pd.read_csv(url_test,error_bad_lines=False, encoding='cp1252',header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOBRGHisC5Q_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "f978a237-42ce-4c02-a378-f2c030b168d0"
      },
      "source": [
        "df_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One more pseudo generalization and I'm giving up.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>One more pseudo generalization or I'm giving up.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The more we study verbs the crazier they get.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Day by day the facts are getting murkier.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I'll fix you a drink.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4785</th>\n",
              "      <td>My name is Astika</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4786</th>\n",
              "      <td>I am Astika</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4787</th>\n",
              "      <td>I is Astika</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4788</th>\n",
              "      <td>I took your umbrella</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4789</th>\n",
              "      <td>I take your Umberella</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4790 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      0  1\n",
              "0     One more pseudo generalization and I'm giving up.  1\n",
              "1      One more pseudo generalization or I'm giving up.  1\n",
              "2         The more we study verbs the crazier they get.  1\n",
              "3             Day by day the facts are getting murkier.  1\n",
              "4                                 I'll fix you a drink.  1\n",
              "...                                                 ... ..\n",
              "4785                                  My name is Astika  1\n",
              "4786                                        I am Astika  1\n",
              "4787                                        I is Astika  0\n",
              "4788                               I took your umbrella  1\n",
              "4789                              I take your Umberella  0\n",
              "\n",
              "[4790 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY7Ns4y2C67D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train=df_train.values\n",
        "data_test=df_test.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNDwgJTUC-Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = data_train[:,0]\n",
        "Y_train = data_train[:,1]\n",
        "X_test = data_test[:,0]\n",
        "Y_test = data_test[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da1vVH6JDVu5",
        "colab_type": "text"
      },
      "source": [
        "# Learn Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-0gRoT1DALh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer_obj=Tokenizer()\n",
        "total_reviews=X_train+X_test\n",
        "tokenizer_obj.fit_on_texts(total_reviews)\n",
        "\n",
        "#pad sequences\n",
        "max_length=max([len(s.split())for s in total_reviews])\n",
        "\n",
        "#define vocabulary size\n",
        "vocab_size=len(tokenizer_obj.word_index)+1\n",
        "\n",
        "X_train_tokens=tokenizer_obj.texts_to_sequences(X_train)\n",
        "X_test_tokens=tokenizer_obj.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad=pad_sequences(X_train_tokens,maxlen=max_length,padding='post')\n",
        "X_test_pad=pad_sequences(X_test_tokens,maxlen=max_length,padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En182gRsDDyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4ed8d86f-98b0-4dcf-f4f8-f411942193ee"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "EMBEDDING_DIM=100\n",
        "\n",
        "print(\"Building Model...\")\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocab_size,EMBEDDING_DIM, input_length=max_length))\n",
        "model.add(GRU(units=32,dropout=0.2,recurrent_dropout=0.2))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building Model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac-8hm9vDGsT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "outputId": "58dc37f0-5ccb-4d5e-f414-b30b2fd6e0eb"
      },
      "source": [
        "print('Training...')\n",
        "\n",
        "model.fit(X_train_pad,Y_train,batch_size=128,epochs=25,validation_data=(X_test_pad,Y_test),verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4790 samples, validate on 4790 samples\n",
            "Epoch 1/25\n",
            " - 3s - loss: 0.6215 - accuracy: 0.7004 - val_loss: 0.6102 - val_accuracy: 0.7025\n",
            "Epoch 2/25\n",
            " - 2s - loss: 0.6118 - accuracy: 0.7023 - val_loss: 0.6117 - val_accuracy: 0.7025\n",
            "Epoch 3/25\n",
            " - 2s - loss: 0.6103 - accuracy: 0.7023 - val_loss: 0.6124 - val_accuracy: 0.7025\n",
            "Epoch 4/25\n",
            " - 2s - loss: 0.6096 - accuracy: 0.7023 - val_loss: 0.6098 - val_accuracy: 0.7025\n",
            "Epoch 5/25\n",
            " - 2s - loss: 0.6091 - accuracy: 0.7023 - val_loss: 0.6095 - val_accuracy: 0.7025\n",
            "Epoch 6/25\n",
            " - 2s - loss: 0.6087 - accuracy: 0.7023 - val_loss: 0.6122 - val_accuracy: 0.7025\n",
            "Epoch 7/25\n",
            " - 2s - loss: 0.6096 - accuracy: 0.7023 - val_loss: 0.6110 - val_accuracy: 0.7025\n",
            "Epoch 8/25\n",
            " - 2s - loss: 0.6107 - accuracy: 0.7023 - val_loss: 0.6121 - val_accuracy: 0.7025\n",
            "Epoch 9/25\n",
            " - 2s - loss: 0.6094 - accuracy: 0.7023 - val_loss: 0.6115 - val_accuracy: 0.7025\n",
            "Epoch 10/25\n",
            " - 2s - loss: 0.6106 - accuracy: 0.7023 - val_loss: 0.6095 - val_accuracy: 0.7025\n",
            "Epoch 11/25\n",
            " - 2s - loss: 0.6092 - accuracy: 0.7023 - val_loss: 0.6115 - val_accuracy: 0.7025\n",
            "Epoch 12/25\n",
            " - 2s - loss: 0.6094 - accuracy: 0.7023 - val_loss: 0.6101 - val_accuracy: 0.7025\n",
            "Epoch 13/25\n",
            " - 2s - loss: 0.6104 - accuracy: 0.7023 - val_loss: 0.6096 - val_accuracy: 0.7025\n",
            "Epoch 14/25\n",
            " - 2s - loss: 0.6106 - accuracy: 0.7023 - val_loss: 0.6095 - val_accuracy: 0.7025\n",
            "Epoch 15/25\n",
            " - 2s - loss: 0.6095 - accuracy: 0.7023 - val_loss: 0.6094 - val_accuracy: 0.7025\n",
            "Epoch 16/25\n",
            " - 2s - loss: 0.6092 - accuracy: 0.7023 - val_loss: 0.6094 - val_accuracy: 0.7025\n",
            "Epoch 17/25\n",
            " - 2s - loss: 0.6095 - accuracy: 0.7023 - val_loss: 0.6089 - val_accuracy: 0.7025\n",
            "Epoch 18/25\n",
            " - 2s - loss: 0.6108 - accuracy: 0.7023 - val_loss: 0.6092 - val_accuracy: 0.7025\n",
            "Epoch 19/25\n",
            " - 2s - loss: 0.6103 - accuracy: 0.7023 - val_loss: 0.6101 - val_accuracy: 0.7025\n",
            "Epoch 20/25\n",
            " - 2s - loss: 0.6099 - accuracy: 0.7023 - val_loss: 0.6096 - val_accuracy: 0.7025\n",
            "Epoch 21/25\n",
            " - 2s - loss: 0.6096 - accuracy: 0.7023 - val_loss: 0.6104 - val_accuracy: 0.7025\n",
            "Epoch 22/25\n",
            " - 2s - loss: 0.6098 - accuracy: 0.7023 - val_loss: 0.6089 - val_accuracy: 0.7025\n",
            "Epoch 23/25\n",
            " - 2s - loss: 0.6097 - accuracy: 0.7023 - val_loss: 0.6088 - val_accuracy: 0.7025\n",
            "Epoch 24/25\n",
            " - 2s - loss: 0.6100 - accuracy: 0.7023 - val_loss: 0.6091 - val_accuracy: 0.7025\n",
            "Epoch 25/25\n",
            " - 2s - loss: 0.6092 - accuracy: 0.7023 - val_loss: 0.6093 - val_accuracy: 0.7025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f5957d3aba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfHrT1ubDKFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f8d0b73f-a02f-4a51-9704-e68beac59f5a"
      },
      "source": [
        "test_samples=[\"Hi how are you\", \"Hi how you\", \"I goes there\"]\n",
        "test_samples_tokens=tokenizer_obj.texts_to_sequences(test_samples)\n",
        "test_samples_tokens_pad=pad_sequences(test_samples_tokens,maxlen=max_length)\n",
        "\n",
        "model.predict(x=test_samples_tokens_pad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5659108],\n",
              "       [0.5722165],\n",
              "       [0.5950344]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b1Q8Gf9DbKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}